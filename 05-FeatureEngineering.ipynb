{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction / Feature Engineering\n",
    "\n",
    "Until now, we just used the following columns: `[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]`. \n",
    "However, there are way more columns. Unluckily, they are strings and cannot be used natively by most machine learning algorithms. Let's see what we can do about that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "As you can see, we have 5 columns that have the type string: `[\"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"]`.\n",
    "\n",
    "Lets start with the `Sex` property, as it is the easiest. To use this in a classifier we can simply encode it using a binary rule: `male` -> 0, `female` -> 1. \n",
    "\n",
    "For that, we first have to select the elements we want to change, which can be done using `df.loc[rows, cols]`. The first parameter specifies the columns which will be selected and the second one the rows. This can be done with a boolean expression and the name of a column, e.g. `df.loc[df[\"Sex\"] == \"male\", \"Sex\"] = 0` selects all rows, where `Sex` is `male` and then sets the corresponding values to `0`.   \n",
    "\n",
    "Let's see how much that improves our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a training and test set again and train another decision tree with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "features = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got way better, just by adding this one feature!\n",
    "Seems like it mattered a lot, if you were male or female. \n",
    "\n",
    "Let's see if we can apply this to other features, e.g. `\"Embarked\"`.\n",
    "\n",
    "It can have four values C = Cherbourg, Q = Queenstown, S = Southampton, for the three ports, where people boarded the Titanic or nothing, if it is uncertain where the person boarded the ship. \n",
    "There are two ways to encode the values. Either as a mapping, C -> 0, Q -> 1, S -> 2 or as three binary features, isC, isQ, isS. The mapping approach saves two feature inputs, but implies a certain ranking for the three ports (This might be valid, if one town has richer occupants than the others). For the missing port we could add another mapping (binary columns) or we could assign it the most occuring port.\n",
    "\n",
    "Let us go with binary columns for the time being and add one for the port missing as well. pandas has a super convenient function for this `pd.get_dummies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train yet another algorithm with the new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\", \"is_C\", \"is_Q\", \"is_S\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "#### Build additional features:\n",
    "Data may contain information that is hard, or even impossible to grasp for a machine learning algorithm, for example the importance of a certain title, public holidays which occur on different dates, sudden changes in human decision making (e.g. the transition from child to young adult)... This information has to be provided explicitly to the algorithm as an extra feature, as it does not have common knowledge. This important Data Science process is called \"Feature Engineering\", and often makes the difference between a good and a bad model.\n",
    "\n",
    "#### In our case, the following extra features could be added:\n",
    "- The title of each person might correlate with their survival probabillity. (e.g. Mr. most unlikely, Miss more likely, Countess most likely,...)\n",
    "- The size of a family might matter as these people are likely to help each other. This is however currently split up between two columns `SibSp` and `Parch`, we could merge them to a `FamilySize` column by adding them to each other.\n",
    "- In earlier times, wealthier people often had longer names, so this could be another feature.\n",
    "- The change from being a child to being an adult, as people are more likely to give the spot in a rescue boat to a child (combination of `Title`, `Age`, and `Sex`)\n",
    "- The number of relatives on the boat, as those are people which are likely to help each other.\n",
    "- An `ID` for each family on the boat. If one member survives, the are more likely to be in the same rescue boat.\n",
    "- If `NaN` values are replaced, we can add the feature col Missing\n",
    "- etc.\n",
    "\n",
    "Let us starting the process of feature engineering by calculating a family size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was easy right? Let's continue with feature engineering on the `Name` column. For that let us look at some examples from that column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Name\"].iloc[888])\n",
    "print(df[\"Name\"].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Name` property is a bit more complex. Besides normal names, it contains rows with names in brackets and with double quotes.\n",
    "\n",
    "In case of double quotes, a nickname is provided, e.g. `Johnston, Miss. Catherine Helen \"Carrie\"`. This could be saved in a binary property as having a nickname or not. Afterwards, the brackets and their contents can be removed.\n",
    "\n",
    "In case of no double quotes, an old-fashioned way of referring to women is used, e.g. `Cumings, Mrs. John Bradley (Florence Briggs Thayer)` refers to the wife of `Mr. John Bradley Cumings` with the maiden name `Florence Briggs Thayer`. This can help for filling missing ages, if one assumes that married couples have similar ages (can be tested with the data). Similarly, the titles `Master` (title of male children) and `Miss` (unmarried women) should be younger than the average.\n",
    "\n",
    "Additionally, we can figure out which people belong to a family by setting a family id made up from the family name and the number of siblings, spouses, parents and children that or on board. As relatives normally stick together closely, it might be reasonable that certain family ids have a higher probability of survival than others.\n",
    "\n",
    "Let us start by extracting the title and assigning value to it which might correlate with their survival probabillity. (e.g. Mr. most unlikely, Miss more likely, Countess most likely,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some helper functions\n",
    "import re\n",
    "\n",
    "titleMapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \n",
    "                \"Dr\": 5, \"Rev\": 6, \"Major\": 7,\n",
    "                \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \n",
    "                \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \n",
    "                \"Capt\": 7, \"Ms\": 2}\n",
    "\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  \n",
    "    # Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    titleSearch = re.search('([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if titleSearch:\n",
    "        for k,v in titleMapping.items():\n",
    "            if k==titleSearch.group(1) : return v\n",
    "    return 0\n",
    "\n",
    "def test_get_title(name_with_title: str, title_value: str):\n",
    "    assert get_title(name_with_title) == title_value\n",
    "    \n",
    "name_title_map = {\n",
    "    \"Braund, Mr. Owen Harris\": 1,\n",
    "    \"Johnston, Miss. Catherine Helen \\\"Carrie\\\"\": 2,\n",
    "    \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\": 3\n",
    "}\n",
    "for k, v in name_title_map.items():\n",
    "    test_get_title(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us continuing by assigining a familyId to people. This should be the same id if the family is the same and the sum of `FamilySize` is equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import operator\n",
    "\n",
    "def get_family_id(row: pd.Series, family_id_map: Dict[str, int]) -> int:\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # Create the family id\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # Look up the id in the mapping\n",
    "    if family_id not in family_id_map:\n",
    "        if len(family_id_map) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "            current_id = (max(family_id_map.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_map[family_id] = current_id\n",
    "    return family_id_map[family_id]\n",
    "\n",
    "def test_get_family_id():\n",
    "    family_id_map = {\n",
    "        \"Cummings5\": 1,\n",
    "        \"Cummings3\": 2\n",
    "    }\n",
    "    test_df = pd.DataFrame(\n",
    "        columns=[\"Name\", \"FamilySize\"], \n",
    "        data = [\n",
    "            (\"Cummings\", 5),\n",
    "            (\"Cummings\", 4),\n",
    "            (\"Cummings\", 3),\n",
    "            (\"Johnston\", 5),\n",
    "        ])\n",
    "    expected_results = [1, 3, 2, 4]\n",
    "    assert [get_family_id(row, family_id_map) for _, row in test_df.iterrows()] == expected_results\n",
    "\n",
    "test_get_family_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the same transformations on training and test data we should pack all of the preporcessing into a function which we can simply pass a dataframe to and it does all these transformations for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessDf(frame):\n",
    "    # Start by encoding the Sex as a number\n",
    "    \n",
    "    # Calculate the FamiliySize\n",
    "    \n",
    "    # Extract the embarked feature using `get_dummies`\n",
    "    \n",
    "    # Extract the title using above given function and using `.apply`\n",
    "    \n",
    "    # Extract the FamilyId using above given function and using `.apply`\n",
    "    \n",
    "    return frame\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df = preprocessDf(df)\n",
    "# The properties Name, Ticket and Cabin are not needed any longer\n",
    "df.drop(['Name', 'Ticket', 'Cabin'], axis = 1, inplace = True, errors = 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Now that we have our feature set, let us actually investigate the data similar to the investigation in the [numerical exploration](./02-ExplorationNumeric.ipynb). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this knowledge about the feature importance,\n",
    "we can either revisit the step of the data preprocessing or start developing a machine learning algorithm (also called a \"classifier\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us train our model again\n",
    "\n",
    "There is a huge variety of models which can be selected, e.g. Linear Regression, Logistic Regression, Decision Trees, Random Forests, Gradient Boost Trees, and Neural Networks. Most of these classifiers are implemented in the `sklearn` package. \n",
    "\n",
    "Let's try a Logistic Regression, a Decision Tree, a Random Forest, and a Gradient Boost Tree, and then have a look how they compare. \n",
    "\n",
    "Let us apply the knowledge of [splitting datasets](./04-TrainTestSplit.ipynb) to first get a training and test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us [train an algortihm](./03-SklearnIntroduction.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this gives us only a starting point about which direction to go, because we just ran all the classifiers with a subset of their parameters. Most classifiers are very sensitive to changes of those parameters. Therefore, it is good practice to read the documentation and statistical background before using new classifiers and interpreting their results.\n",
    "\n",
    "To test how the models handle different parameter sets, we can use `GridSearch`. This evaluates the model with multiple sets of parameters. However, this can lead to excessive computation time (good hardware and parallel computing help though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "paramGrid = {'max_depth': [1,2,3,8,None],\n",
    "            'max_features': [1,2,4,8,None]}\n",
    "grid = GridSearchCV(clf, param_grid=paramGrid, cv=5)\n",
    "grid.fit(xTrain, yTrain)\n",
    "print(grid.score(xTest, yTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.matshow(np.array(grid.cv_results_[\"mean_test_score\"]).reshape(5, -1))\n",
    "plt.xlabel(\"depth\")\n",
    "plt.ylabel(\"max features\")\n",
    "plt.xticks(range(len(paramGrid['max_depth'])), paramGrid['max_depth'])\n",
    "plt.yticks(range(len(paramGrid['max_features'])), paramGrid['max_features'])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is your time to change the code above to figure out how good you can get."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
